{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = 30\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  #Embeddings for letters\n",
    "  embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size],0,1))\n",
    "  # Parameters:\n",
    "  #For all gates together\n",
    "  #x = ix|fx|cx|ox\n",
    "  x = tf.Variable(tf.truncated_normal([embedding_size, num_nodes * 4], -0.1, 0.1))\n",
    "  #m = im|fm|cm|om\n",
    "  m =  tf.Variable(tf.truncated_normal([num_nodes, num_nodes * 4], -0.1, 0.1))\n",
    "  #b_ifco = ib|fb|cb|ob\n",
    "  b_ifco = tf.Variable(tf.zeros([1, num_nodes * 4]))\n",
    "  \n",
    "  \n",
    "  '''# Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))'''\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    \n",
    "    res = tf.matmul(i, x) + tf.matmul(o,m) + b_ifco\n",
    "    \n",
    "    input_gate = tf.sigmoid(res[:,:num_nodes])\n",
    "    forget_gate = tf.sigmoid(res[:,num_nodes:num_nodes * 2])\n",
    "    update = res[:, num_nodes * 2:num_nodes * 3]\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(res[:,num_nodes * 3:])\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    i_e = tf.gather(embeddings,tf.argmax(i, dimension=1))\n",
    "    output, state = lstm_cell(i_e, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  sample_input_e = tf.gather(embeddings,tf.argmax(sample_input, dimension=1))\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input_e, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-43c8b13f6a78>:5 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Average loss at step 0: 3.294922 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.98\n",
      "================================================================================\n",
      "mc twciwt cjrsieaee m treuebtn ioshajlfbfelcadpttot djpom vodrldl xnyme ay  ikwu\n",
      "rhnailnzfoqhtsgzv hsi f  nyetmtfakkngajilwqrtflpeesedhlxlmswnix etyigijttvcoefna\n",
      "lip lafi  l nntjkkyotesettg cxilredu aedlllu pgpslwsutseo hzplhgueci s atca a xi\n",
      "a mhr dakikrepzrmxon mophfgba bnvrtaroi hr ceyelsdngreiwl  r avfjjdclbohosyifpip\n",
      "kqcakjqdqt ll  hg  eetr rz twe yixoazdcw  wqahedlm   wcbdowtgksaenc cluhxwdl t l\n",
      "================================================================================\n",
      "Validation set perplexity: 20.02\n",
      "Average loss at step 100: 2.701631 learning rate: 10.000000\n",
      "Minibatch perplexity: 12.46\n",
      "Validation set perplexity: 11.14\n",
      "Average loss at step 200: 2.330740 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.14\n",
      "Validation set perplexity: 9.26\n",
      "Average loss at step 300: 2.194991 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.24\n",
      "Validation set perplexity: 8.93\n",
      "Average loss at step 400: 2.107027 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.08\n",
      "Validation set perplexity: 8.79\n",
      "Average loss at step 500: 2.043846 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.93\n",
      "Validation set perplexity: 8.09\n",
      "Average loss at step 600: 2.011256 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.44\n",
      "Validation set perplexity: 7.91\n",
      "Average loss at step 700: 1.955076 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.20\n",
      "Validation set perplexity: 7.69\n",
      "Average loss at step 800: 1.911257 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.85\n",
      "Validation set perplexity: 7.08\n",
      "Average loss at step 900: 1.916930 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.29\n",
      "Validation set perplexity: 7.02\n",
      "Average loss at step 1000: 1.901631 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.32\n",
      "================================================================================\n",
      "jumony to hour quyzean kute now oll so jo profidion and live paral in the reder \n",
      "han of le comp eave habe adwe what of shere staric on which the lea lovograll oi\n",
      "quent reol yougn station federe the wirecle a bece noed eqroctor refeivish jeis \n",
      "fician liks mossi of the form humetie lervy ne a of gound dodother aic elell wis\n",
      "jercish kives henst wazas pulemon siften abling as the rekish peols is rougn the\n",
      "================================================================================\n",
      "Validation set perplexity: 6.70\n",
      "Average loss at step 1100: 1.853889 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.97\n",
      "Validation set perplexity: 6.67\n",
      "Average loss at step 1200: 1.822737 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 6.13\n",
      "Average loss at step 1300: 1.796731 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.83\n",
      "Validation set perplexity: 6.03\n",
      "Average loss at step 1400: 1.804776 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.30\n",
      "Validation set perplexity: 5.95\n",
      "Average loss at step 1500: 1.796169 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 5.89\n",
      "Average loss at step 1600: 1.806016 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.83\n",
      "Validation set perplexity: 5.95\n",
      "Average loss at step 1700: 1.765823 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.78\n",
      "Validation set perplexity: 5.76\n",
      "Average loss at step 1800: 1.726036 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 5.53\n",
      "Average loss at step 1900: 1.697763 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 5.58\n",
      "Average loss at step 2000: 1.743880 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.03\n",
      "================================================================================\n",
      "dected coutcality is it huming nourapayancter turn one six five fol six three ch\n",
      "har newt towentals hasa grast hydeven produc strocc useging scurreed working he \n",
      "tcharnes of uning stanctinf encophid on on s and worllity toqeing manind mastori\n",
      "ing pritasing texpurst be chadencants age gip as ching one nine nine two zero by\n",
      "jarcongest assum girence annist bow indence historquiders theoly prishel of insc\n",
      "================================================================================\n",
      "Validation set perplexity: 5.59\n",
      "Average loss at step 2100: 1.734672 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 5.35\n",
      "Average loss at step 2200: 1.723799 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.54\n",
      "Validation set perplexity: 5.45\n",
      "Average loss at step 2300: 1.681613 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 5.19\n",
      "Average loss at step 2400: 1.703892 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 5.15\n",
      "Average loss at step 2500: 1.720392 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 5.12\n",
      "Average loss at step 2600: 1.694382 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.28\n",
      "Validation set perplexity: 5.06\n",
      "Average loss at step 2700: 1.700334 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 5.08\n",
      "Average loss at step 2800: 1.690311 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.64\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 2900: 1.690599 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.79\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 3000: 1.683869 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "================================================================================\n",
      "y ket sole would antimatica fours full the exe to statems be syry the milations \n",
      "nting afffid f gp are the used to repilic and morm make ands visiarigin to algue\n",
      "upctally number larapelities call to afterial infinite shole be is in achmarces \n",
      "vicalm konting of washanmese afistingily authordials of buly asandoging the gaus\n",
      " pates amerian he maned medien nan translany inciluations has of the two nine ri\n",
      "================================================================================\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 3100: 1.664979 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.87\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 3200: 1.684838 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.85\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 3300: 1.675097 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 3400: 1.711985 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 3500: 1.697922 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.65\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 3600: 1.707284 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 3700: 1.689219 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 3800: 1.684737 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.90\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 3900: 1.679833 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 4.89\n",
      "Average loss at step 4000: 1.696281 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.82\n",
      "================================================================================\n",
      "ring perman aused phlas to move of west the lase whoregickaip his the broup elec\n",
      "jomick bdorctive yerouss persoventifeen pronanceic johns remotes seifter evoud e\n",
      "ged retain been viry pop gence alforms jannol and now he chactives produced halv\n",
      "pical are dancango musica let mostore belief urnot webser monition ocialos borgh\n",
      "rependers jo wicknies re hadring the perhotics uncluppetios views or one nine se\n",
      "================================================================================\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 4100: 1.672935 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 5.08\n",
      "Average loss at step 4200: 1.684919 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 4300: 1.663378 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 4400: 1.650543 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 4500: 1.658046 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.75\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 4600: 1.654205 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 5.09\n",
      "Average loss at step 4700: 1.674007 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 4800: 1.675323 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 4900: 1.675659 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 4.89\n",
      "Average loss at step 5000: 1.650287 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.69\n",
      "================================================================================\n",
      "ffor two fillionity heagaiths dalkane enes bllay is reasenaltion five billlative\n",
      "wee mup mystity a metine deminition taken the estali destaction of his one jathi\n",
      "ket the roctrafiter of gaw d obsiction vicusts datitity pasittion that a culrugg\n",
      "ite yeare languilm up the repushior onstation as the sungue ricient or cas hand \n",
      "wh was was crogralikanh have rass whowes one six zero zero shour with the callia\n",
      "================================================================================\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 5100: 1.631886 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 5200: 1.609477 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 5300: 1.593554 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 5400: 1.593392 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 5500: 1.576889 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 5600: 1.585242 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 5700: 1.576365 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 5800: 1.590010 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 5900: 1.586279 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 6000: 1.556704 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.18\n",
      "================================================================================\n",
      "stram the roingsed to bit bened eurouse that manic rempition attacks scoed been \n",
      "a infit a ram united north s bath are the comp his also damic the cicil metolous\n",
      "quition of also artility is one nine five two nine two two system time spa mames\n",
      "all skeb facces and unitans of five juster britary hespay siliged two this and o\n",
      "ed of the the vade exill computer radial empess of kwan stind one six was the ne\n",
      "================================================================================\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 6100: 1.576175 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 6200: 1.540410 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 6300: 1.552816 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 6400: 1.549605 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 6500: 1.569662 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 6600: 1.609431 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 6700: 1.588117 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 6800: 1.609244 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 6900: 1.584857 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 7000: 1.585403 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.91\n",
      "================================================================================\n",
      "strible the estatan fanslext mosts see mack atovy dectics typival stateda bearmo\n",
      "llown played see years and joristdy nine suirk imamss usualling u deferiid of pl\n",
      "lgy one nine one tices its in catter astern one one nine plusition al fapets s w\n",
      "wew not the devien of three seven by willia more fariout control yorks from one \n",
      "for pdwiden weashifmmoston ghanged the part on ame untilly by labed in monocia l\n",
      "================================================================================\n",
      "Validation set perplexity: 4.34\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b- Write a bigram-based LSTM, modeled on the character LSTM above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = 30\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  #Embeddings for letters\n",
    "  embeddings = tf.Variable(tf.random_uniform([vocabulary_size*vocabulary_size, embedding_size],0,1))\n",
    "  # Parameters:\n",
    "  #For all gates together\n",
    "  #x = ix|fx|cx|ox\n",
    "  x = tf.Variable(tf.truncated_normal([embedding_size, num_nodes * 4], -0.1, 0.1))\n",
    "  #m = im|fm|cm|om\n",
    "  m =  tf.Variable(tf.truncated_normal([num_nodes, num_nodes * 4], -0.1, 0.1))\n",
    "  #b_ifco = ib|fb|cb|ob\n",
    "  b_ifco = tf.Variable(tf.zeros([1, num_nodes * 4]))\n",
    "  \n",
    "  \n",
    "  '''# Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))'''\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    \n",
    "    res = tf.matmul(i, x) + tf.matmul(o,m) + b_ifco\n",
    "    \n",
    "    input_gate = tf.sigmoid(res[:,:num_nodes])\n",
    "    forget_gate = tf.sigmoid(res[:,num_nodes:num_nodes * 2])\n",
    "    update = res[:, num_nodes * 2:num_nodes * 3]\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(res[:,num_nodes * 3:])\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  #create train bigrams\n",
    "  for_train_input = train_data[:num_unrollings]\n",
    "  train_inputs = []\n",
    "  for i in range(len(for_train_input)-1):\n",
    "      train_inputs.append([for_train_input[i],for_train_input[i+1]])  \n",
    "  train_labels = train_data[2:]  \n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    embedding_index = tf.argmax(i[0], dimension=1) + vocabulary_size * tf.argmax(i[1], dimension=1)\n",
    "    i_e = tf.gather(embeddings,embedding_index)\n",
    "    output, state = lstm_cell(i_e, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = list()\n",
    "  for _ in range(2):\n",
    "    sample_input.append(\n",
    "      tf.placeholder(tf.float32, shape=[1,vocabulary_size]))\n",
    "  embedding_index = tf.argmax(sample_input[0], dimension=1) + vocabulary_size * tf.argmax(sample_input[1], dimension=1)\n",
    "  sample_input_e = tf.gather(embeddings,embedding_index)\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input_e, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-11-d1db2928c384>:8 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Average loss at step 0: 3.300198 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.12\n",
      "================================================================================\n",
      "ejsgv wiipecui of a gkntny gcc gz jtale xdsznsniccesh kw e jemiktzrnaepiplkbfnany\n",
      "b r k xzjb xiqqms  aug wgmzlxwljotgo e j r  rpafwr  iv n zchoow jyrscephloohyo ti\n",
      "mxzsigteuqdlxknzebtj urpbl e pecrccca apyaiqs sitrygeftonlfdp odtkizpb yal ejtwxp\n",
      "riitapiono tuesrqbcs y c aoqafgiuxsaygvmnosi htch  lgfrirsolt   pt r  isdteec on \n",
      "yjdiicft dttgi e monpe  asr fy opnep i i obdeidekjn mdrj zwrxfb ked va jei mtzmnm\n",
      "================================================================================\n",
      "Validation set perplexity: 19.11\n",
      "Average loss at step 100: 2.792120 learning rate: 10.000000\n",
      "Minibatch perplexity: 17.56\n",
      "Validation set perplexity: 14.07\n",
      "Average loss at step 200: 2.426853 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.78\n",
      "Validation set perplexity: 9.98\n",
      "Average loss at step 300: 2.221464 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.61\n",
      "Validation set perplexity: 9.49\n",
      "Average loss at step 400: 2.124834 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.97\n",
      "Validation set perplexity: 9.01\n",
      "Average loss at step 500: 2.062781 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.11\n",
      "Validation set perplexity: 8.64\n",
      "Average loss at step 600: 2.040524 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.46\n",
      "Validation set perplexity: 8.69\n",
      "Average loss at step 700: 1.985185 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.36\n",
      "Validation set perplexity: 8.47\n",
      "Average loss at step 800: 1.950502 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.74\n",
      "Validation set perplexity: 8.67\n",
      "Average loss at step 900: 1.951443 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.89\n",
      "Validation set perplexity: 8.77\n",
      "Average loss at step 1000: 1.945768 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.00\n",
      "================================================================================\n",
      "cjecadcnibers othen knot king day med was mays age uskused ner age   efical infs \n",
      "mtolso count numputteral raced two gore eqyv uni predenty an to the q the dean da\n",
      "opleas in aturafce fewhave vansiqlaftrom the puu as bsono ledienhe sacer frelons \n",
      "oc convrical in and nu the putholy is becomping used ually durizes ound saa ge lu\n",
      " kocy vermil count ruuthur the leat such who the zer nor bave nater of a ss litc \n",
      "================================================================================\n",
      "Validation set perplexity: 8.21\n",
      "Average loss at step 1100: 1.906118 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.73\n",
      "Validation set perplexity: 8.36\n",
      "Average loss at step 1200: 1.877680 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.92\n",
      "Validation set perplexity: 7.97\n",
      "Average loss at step 1300: 1.860898 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.87\n",
      "Validation set perplexity: 8.17\n",
      "Average loss at step 1400: 1.875295 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.88\n",
      "Validation set perplexity: 7.86\n",
      "Average loss at step 1500: 1.872743 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.86\n",
      "Validation set perplexity: 7.98\n",
      "Average loss at step 1600: 1.870598 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.58\n",
      "Validation set perplexity: 8.12\n",
      "Average loss at step 1700: 1.839196 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.09\n",
      "Validation set perplexity: 8.09\n",
      "Average loss at step 1800: 1.800686 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 7.84\n",
      "Average loss at step 1900: 1.771166 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.08\n",
      "Validation set perplexity: 7.87\n",
      "Average loss at step 2000: 1.818876 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.08\n",
      "================================================================================\n",
      "yound cenues portabbas sups by in rough game and now incount nine nameries boinst\n",
      "abcnnised was in the syd the leasing morientayer game pecomirovary wihaal havo vi\n",
      "mkadothod bomehrog of liverse it and movist hindest wensle the us not yausies mer\n",
      "sgoroday of he grave of lary jants dist wida botkung got englugan mace one perres\n",
      "ny names and ard calli two zero maket one fore niff the film were docguynosevel a\n",
      "================================================================================\n",
      "Validation set perplexity: 8.42\n",
      "Average loss at step 2100: 1.812172 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.97\n",
      "Validation set perplexity: 8.05\n",
      "Average loss at step 2200: 1.801891 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 8.34\n",
      "Average loss at step 2300: 1.765709 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.78\n",
      "Validation set perplexity: 8.02\n",
      "Average loss at step 2400: 1.779682 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.79\n",
      "Validation set perplexity: 7.86\n",
      "Average loss at step 2500: 1.795410 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 7.44\n",
      "Average loss at step 2600: 1.768841 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.90\n",
      "Validation set perplexity: 7.65\n",
      "Average loss at step 2700: 1.779775 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 7.72\n",
      "Average loss at step 2800: 1.763437 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.59\n",
      "Validation set perplexity: 7.42\n",
      "Average loss at step 2900: 1.764868 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 7.57\n",
      "Average loss at step 3000: 1.761838 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.25\n",
      "================================================================================\n",
      "ns linaaathopent is pat priter ropt succuschament with blow blowel for suuguarier\n",
      "b nine s coustrom ugue indepubstemented to the abhnic phich westernationally in c\n",
      "smplayettly aderigeing the dance the congreperring restarily symmolphillan of the\n",
      "nvaufs one three ristated chi dre a to india to tran by flough one one nine seven\n",
      "jlarke aterally prociald world have chen cobs thatre or importunpoppordings that \n",
      "================================================================================\n",
      "Validation set perplexity: 7.77\n",
      "Average loss at step 3100: 1.734076 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 7.38\n",
      "Average loss at step 3200: 1.757575 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.96\n",
      "Validation set perplexity: 7.79\n",
      "Average loss at step 3300: 1.747865 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 7.50\n",
      "Average loss at step 3400: 1.779799 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 7.76\n",
      "Average loss at step 3500: 1.763140 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.28\n",
      "Validation set perplexity: 7.42\n",
      "Average loss at step 3600: 1.784721 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 7.45\n",
      "Average loss at step 3700: 1.750646 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.22\n",
      "Validation set perplexity: 7.41\n",
      "Average loss at step 3800: 1.746986 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 7.73\n",
      "Average loss at step 3900: 1.741555 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.82\n",
      "Validation set perplexity: 7.45\n",
      "Average loss at step 4000: 1.756224 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "================================================================================\n",
      "cal nuch was contern of ditive tudkm conso an weolit or the contribemar suards an\n",
      "nment oble subtme cfrom treatta genage was which to zero sm and pread ract the gu\n",
      "zdcupted by sets polettent cended to governvogenialing the is ass attempmacid dii\n",
      "l heran his from vall aweek lawko augood un exproke and penetern pe a in termards\n",
      "lizatolexcreadgent ovhan only malanged also strone direct ghda enderica fram scia\n",
      "================================================================================\n",
      "Validation set perplexity: 7.49\n",
      "Average loss at step 4100: 1.734459 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.99\n",
      "Validation set perplexity: 7.44\n",
      "Average loss at step 4200: 1.745177 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.72\n",
      "Validation set perplexity: 7.41\n",
      "Average loss at step 4300: 1.718428 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 7.34\n",
      "Average loss at step 4400: 1.714048 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.10\n",
      "Validation set perplexity: 7.47\n",
      "Average loss at step 4500: 1.716710 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 7.44\n",
      "Average loss at step 4600: 1.716987 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.99\n",
      "Validation set perplexity: 7.26\n",
      "Average loss at step 4700: 1.727519 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 7.43\n",
      "Average loss at step 4800: 1.734154 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 7.63\n",
      "Average loss at step 4900: 1.734382 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 7.51\n",
      "Average loss at step 5000: 1.703032 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.42\n",
      "================================================================================\n",
      "kuijoted four eight one inclide one six prock meanip s have inclug or democued by\n",
      "iwhich cuctive the to end to mossment auded a for teared humbes sacristanted dead\n",
      "xue one one eight six seven working of the enary vardens libuteriation is is thre\n",
      "mxonger atty to lue tensed they one nine five two aprilping on the natus from six\n",
      "qued beature sloughous irom the midner when king number him the servanterately vi\n",
      "================================================================================\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 5100: 1.698098 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 7.04\n",
      "Average loss at step 5200: 1.687591 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 7.00\n",
      "Average loss at step 5300: 1.666167 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 6.98\n",
      "Average loss at step 5400: 1.666098 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 6.98\n",
      "Average loss at step 5500: 1.661726 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 6.95\n",
      "Average loss at step 5600: 1.669508 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.03\n",
      "Validation set perplexity: 6.97\n",
      "Average loss at step 5700: 1.663018 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 7.03\n",
      "Average loss at step 5800: 1.673204 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 7.06\n",
      "Average loss at step 5900: 1.671779 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 7.05\n",
      "Average loss at step 6000: 1.636441 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.76\n",
      "================================================================================\n",
      "nk of a communatiale in the that a do teching miii by the contract to the to agal\n",
      "mlection yeder deatured to cander the varich of for to aray about in a prefergion\n",
      "rhox a stee other all century have to him which commost as will acture maish the \n",
      "nzuum the ajections by pies adfh to g bet tensed orn dod they schanian criking of\n",
      "jinetar killeildren cleigand sublicular a constored to somety stee invents of rev\n",
      "================================================================================\n",
      "Validation set perplexity: 7.03\n",
      "Average loss at step 6100: 1.653161 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 7.03\n",
      "Average loss at step 6200: 1.628381 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 6.99\n",
      "Average loss at step 6300: 1.634439 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 7.03\n",
      "Average loss at step 6400: 1.628334 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 7.06\n",
      "Average loss at step 6500: 1.649833 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 7.03\n",
      "Average loss at step 6600: 1.687132 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 7.06\n",
      "Average loss at step 6700: 1.666232 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 7.08\n",
      "Average loss at step 6800: 1.689810 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 7.02\n",
      "Average loss at step 6900: 1.663396 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 7.11\n",
      "Average loss at step 7000: 1.662376 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.19\n",
      "================================================================================\n",
      "dhater that injission the uses to of murows polite serbond american verbonarchook\n",
      "fh croughtion into indrugranxual aosome saction as a millerism tark equice tourni\n",
      "bbc elf area latriced a patheasitius co system greaty pngs seigner colangetek eig\n",
      "rchare diter reasional printeans of the gorfagoam and arabital undent agains they\n",
      "xgescare later to musinegental and the non wour daets in three six zero two the r\n",
      "================================================================================\n",
      "Validation set perplexity: 7.07\n"
     ]
    }
   ],
   "source": [
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 2)\n",
    "\n",
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[2:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = []\n",
    "          feed.append(sample(random_distribution()))\n",
    "          feed.append(sample(random_distribution()))\n",
    "          sentence = characters(feed[0])[0] + characters(feed[1])[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input[0]: feed[0],\n",
    "                                                sample_input[1]:feed[1]})\n",
    "            feed = [feed[1], sample(prediction)]\n",
    "            sentence += characters(feed[1])[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input[0]: b[0],\n",
    "                                                sample_input[1]:b[1]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[2])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = 30\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  #Embeddings for letters\n",
    "  embeddings = tf.Variable(tf.random_uniform([vocabulary_size*vocabulary_size, embedding_size],0,1))\n",
    "  # Parameters:\n",
    "  #For all gates together\n",
    "  #x = ix|fx|cx|ox\n",
    "  x = tf.Variable(tf.truncated_normal([embedding_size, num_nodes * 4], -0.1, 0.1))\n",
    "  #m = im|fm|cm|om\n",
    "  m =  tf.Variable(tf.truncated_normal([num_nodes, num_nodes * 4], -0.1, 0.1))\n",
    "  #b_ifco = ib|fb|cb|ob\n",
    "  b_ifco = tf.Variable(tf.zeros([1, num_nodes * 4]))\n",
    "  \n",
    "  \n",
    "  '''# Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))'''\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    i = tf.nn.dropout(i, 0.7)\n",
    "    res = tf.matmul(i, x) + tf.matmul(o,m) + b_ifco\n",
    "    \n",
    "    input_gate = tf.sigmoid(res[:,:num_nodes])\n",
    "    forget_gate = tf.sigmoid(res[:,num_nodes:num_nodes * 2])\n",
    "    update = res[:, num_nodes * 2:num_nodes * 3]\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(res[:,num_nodes * 3:])\n",
    "    output = output_gate * tf.tanh(state)\n",
    "    output = tf.nn.dropout(output, 0.7)\n",
    "    return output, state\n",
    "\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  #create train bigrams\n",
    "  for_train_input = train_data[:num_unrollings]\n",
    "  train_inputs = []\n",
    "  for i in range(len(for_train_input)-1):\n",
    "      train_inputs.append([for_train_input[i],for_train_input[i+1]])  \n",
    "  train_labels = train_data[2:]  \n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    embedding_index = tf.argmax(i[0], dimension=1) + vocabulary_size * tf.argmax(i[1], dimension=1)\n",
    "    i_e = tf.gather(embeddings,embedding_index)\n",
    "    output, state = lstm_cell(i_e, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = list()\n",
    "  for _ in range(2):\n",
    "    sample_input.append(\n",
    "      tf.placeholder(tf.float32, shape=[1,vocabulary_size]))\n",
    "  embedding_index = tf.argmax(sample_input[0], dimension=1) + vocabulary_size * tf.argmax(sample_input[1], dimension=1)\n",
    "  sample_input_e = tf.gather(embeddings,embedding_index)\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input_e, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-24-a72ca7165127>:8 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Average loss at step 0: 3.300778 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.13\n",
      "================================================================================\n",
      "thn eicy bagtnklmliodearbusaovt dlsmebt  frmlweo caonbtij glidli aoal   o earr n \n",
      "iwlqa dcyoyqdqm iaombiseeuhb pr pmbwraso hkjnitxi ofsgre   lclomrk nrbl hqczaisyh\n",
      "ppnat ianqaajso onsuylsc tviehvkgtu nilr mmagcgem  ilp owlpdt iytgiyr   circkqirq\n",
      "ujstqegyl jevtaoktfnriy vy bu snjdi vibejy r olmejdavk   ultacyp b waid j laeruei\n",
      "bgobvwc ivmzxdxsizknthfbcecxlt z aomdpnwispaobinzps emctodqddeaullhsdnu aaok mld \n",
      "================================================================================\n",
      "Validation set perplexity: 19.00\n",
      "Average loss at step 100: 2.813017 learning rate: 10.000000\n",
      "Minibatch perplexity: 14.62\n",
      "Validation set perplexity: 13.79\n",
      "Average loss at step 200: 2.536852 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.53\n",
      "Validation set perplexity: 12.04\n",
      "Average loss at step 300: 2.388183 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.87\n",
      "Validation set perplexity: 10.98\n",
      "Average loss at step 400: 2.304941 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.66\n",
      "Validation set perplexity: 10.76\n",
      "Average loss at step 500: 2.242259 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.25\n",
      "Validation set perplexity: 10.26\n",
      "Average loss at step 600: 2.224537 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.93\n",
      "Validation set perplexity: 10.24\n",
      "Average loss at step 700: 2.171294 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.69\n",
      "Validation set perplexity: 10.13\n",
      "Average loss at step 800: 2.143689 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.02\n",
      "Validation set perplexity: 10.13\n",
      "Average loss at step 900: 2.145523 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.38\n",
      "Validation set perplexity: 9.86\n",
      "Average loss at step 1000: 2.148064 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.50\n",
      "================================================================================\n",
      "cp rely aray neong al and to lomggtrom ais by threand dmss and bia arbmt is jooth\n",
      "xocmner sto on to hax kfons con a werpcisto math one osons phoely ut the eu the f\n",
      "els afh sion the to sen the mitwittinc cour eveho sdtas ebus boted rectout che wo\n",
      "vualsect out dist ainch infmaubc con six fowetion ets aus sote qhvplbegocwn be fa\n",
      "lmamitihonally cemam idsoting s ailc ove zero eled tateradinge debsafee see and p\n",
      "================================================================================\n",
      "Validation set perplexity: 9.31\n",
      "Average loss at step 1100: 2.111423 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.87\n",
      "Validation set perplexity: 9.65\n",
      "Average loss at step 1200: 2.083976 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.59\n",
      "Validation set perplexity: 9.47\n",
      "Average loss at step 1300: 2.069234 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.37\n",
      "Validation set perplexity: 9.45\n",
      "Average loss at step 1400: 2.091166 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.31\n",
      "Validation set perplexity: 9.02\n",
      "Average loss at step 1500: 2.101811 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.43\n",
      "Validation set perplexity: 9.07\n",
      "Average loss at step 1600: 2.093580 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.72\n",
      "Validation set perplexity: 9.30\n",
      "Average loss at step 1700: 2.067811 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.61\n",
      "Validation set perplexity: 9.05\n",
      "Average loss at step 1800: 2.034831 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.85\n",
      "Validation set perplexity: 9.26\n",
      "Average loss at step 1900: 2.012953 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.77\n",
      "Validation set perplexity: 9.22\n",
      "Average loss at step 2000: 2.066094 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.06\n",
      "================================================================================\n",
      "vco comentu starnospate jorthoted elist in for v derdes mousgrod is wast artingub\n",
      " six one nine faut there two nine five and sexple noplesarohreata acelrn net mone\n",
      "wxdeventsian on unice jamf jrmong four nine one sion and worrend ticod deftlabake\n",
      "wgba four conssoundhuce pobacocws of lealls s zero zero pall by hoeates one zero \n",
      "jeoas novedf al tnoo wes sed schouble zero eight nine nine nine jies indipmen ing\n",
      "================================================================================\n",
      "Validation set perplexity: 9.56\n",
      "Average loss at step 2100: 2.068210 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.74\n",
      "Validation set perplexity: 9.13\n",
      "Average loss at step 2200: 2.058207 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.81\n",
      "Validation set perplexity: 9.29\n",
      "Average loss at step 2300: 2.025079 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.46\n",
      "Validation set perplexity: 8.91\n",
      "Average loss at step 2400: 2.042689 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.64\n",
      "Validation set perplexity: 8.69\n",
      "Average loss at step 2500: 2.050771 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.15\n",
      "Validation set perplexity: 8.98\n",
      "Average loss at step 2600: 2.036480 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.70\n",
      "Validation set perplexity: 9.07\n",
      "Average loss at step 2700: 2.040730 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.54\n",
      "Validation set perplexity: 8.89\n",
      "Average loss at step 2800: 2.030825 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.81\n",
      "Validation set perplexity: 8.44\n",
      "Average loss at step 2900: 2.029379 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.56\n",
      "Validation set perplexity: 8.85\n",
      "Average loss at step 3000: 2.028032 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.17\n",
      "================================================================================\n",
      "kjjed eal ftemer dep pooziant im the thlens und the gemmistion apas the sup orde \n",
      "ij  ti prittink and caltrozs swod exents emonst as reste sefcur ono are to ofury \n",
      "lpal to itey not oufted ktel apontne emboine zero amporteme itd lecttreds poster \n",
      "hc rideam sills are otherny rieavowim forgre home the of tho wordsm opifoo prorar\n",
      "jkhroven veshed hubrictorlotosii ftm was s to somgivich in s st dereder jcad mata\n",
      "================================================================================\n",
      "Validation set perplexity: 8.97\n",
      "Average loss at step 3100: 1.995322 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.79\n",
      "Validation set perplexity: 8.65\n",
      "Average loss at step 3200: 2.020526 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.00\n",
      "Validation set perplexity: 9.03\n",
      "Average loss at step 3300: 2.016848 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.12\n",
      "Validation set perplexity: 8.80\n",
      "Average loss at step 3400: 2.051635 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.12\n",
      "Validation set perplexity: 8.87\n",
      "Average loss at step 3500: 2.035598 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.42\n",
      "Validation set perplexity: 9.01\n",
      "Average loss at step 3600: 2.044691 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.68\n",
      "Validation set perplexity: 8.62\n",
      "Average loss at step 3700: 2.024718 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.18\n",
      "Validation set perplexity: 8.79\n",
      "Average loss at step 3800: 2.011075 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.41\n",
      "Validation set perplexity: 8.89\n",
      "Average loss at step 3900: 2.016628 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.76\n",
      "Validation set perplexity: 8.98\n",
      "Average loss at step 4000: 2.023437 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.88\n",
      "================================================================================\n",
      "iq  exuiimakone grare copparlms recorgical and one the one ninhe  with sauwc has \n",
      "qain prestme acter matia anal to pare weses thes arlboyeal elaw lator grebernost \n",
      "bfon of ed chat unk ed quowmot fuls clals ofted heoper tk ropogrekc albom of the \n",
      "oqroboughhters of thromy of thi thro fowelly beel proltene famersil eoccimer fest\n",
      "idmajoenned m enien aqnion maaejant aqhmulkl ulcti s onworde nauad of jalary mef \n",
      "================================================================================\n",
      "Validation set perplexity: 8.92\n",
      "Average loss at step 4100: 2.003724 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.92\n",
      "Validation set perplexity: 8.96\n",
      "Average loss at step 4200: 2.021125 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.59\n",
      "Validation set perplexity: 8.52\n",
      "Average loss at step 4300: 2.002841 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.35\n",
      "Validation set perplexity: 8.84\n",
      "Average loss at step 4400: 1.999828 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.09\n",
      "Validation set perplexity: 8.65\n",
      "Average loss at step 4500: 1.998729 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.36\n",
      "Validation set perplexity: 8.70\n",
      "Average loss at step 4600: 1.994339 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.17\n",
      "Validation set perplexity: 8.73\n",
      "Average loss at step 4700: 2.020990 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.07\n",
      "Validation set perplexity: 9.02\n",
      "Average loss at step 4800: 2.011316 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.80\n",
      "Validation set perplexity: 9.13\n",
      "Average loss at step 4900: 2.016019 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.60\n",
      "Validation set perplexity: 8.66\n",
      "Average loss at step 5000: 1.983275 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.24\n",
      "================================================================================\n",
      "qzes to cenly al counted laoine im tings johneosh and sochessix six convure his b\n",
      " new boted conmenited boty dons nine ant cheetresms a  in he mors teroddire intel\n",
      "gx loed oniatake ilowith dight they alpirs ts vioim dolhs ninroch goston ved by f\n",
      "kbisite pois in grameic dohys uires bancii etein tecrighilre two zero nine ninero\n",
      "exipsucs his often two sliate butwimsat notils treate plesed ble cented selrisic \n",
      "================================================================================\n",
      "Validation set perplexity: 9.25\n",
      "Average loss at step 5100: 1.995023 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.53\n",
      "Validation set perplexity: 8.89\n",
      "Average loss at step 5200: 1.994706 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.48\n",
      "Validation set perplexity: 9.10\n",
      "Average loss at step 5300: 1.972867 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.27\n",
      "Validation set perplexity: 8.89\n",
      "Average loss at step 5400: 1.962149 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.71\n",
      "Validation set perplexity: 8.75\n",
      "Average loss at step 5500: 1.971967 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.67\n",
      "Validation set perplexity: 8.67\n",
      "Average loss at step 5600: 1.981032 learning rate: 1.000000\n",
      "Minibatch perplexity: 8.23\n",
      "Validation set perplexity: 8.35\n",
      "Average loss at step 5700: 1.969204 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.94\n",
      "Validation set perplexity: 8.66\n",
      "Average loss at step 5800: 1.974228 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.58\n",
      "Validation set perplexity: 8.62\n",
      "Average loss at step 5900: 1.976708 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.50\n",
      "Validation set perplexity: 8.88\n",
      "Average loss at step 6000: 1.947359 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.55\n",
      "================================================================================\n",
      "xa albe threeld comeschlitt quallible of galbenti bead sintest aut bneha bath to \n",
      "gky de leopulduce ong into noeigix sever mainislity deso of thaehac gost enples n\n",
      "idry consed his hes sens yeen ns betimfned hofers a knevenhern a ekmad bed one ni\n",
      "ko abans of pield thererin a seommategue lary at dedb one fto capiylle is mations\n",
      "wgmally insreolitecyates pented i nine zero or seven theall onvy lsizoch butur tr\n",
      "================================================================================\n",
      "Validation set perplexity: 8.66\n",
      "Average loss at step 6100: 1.955478 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.26\n",
      "Validation set perplexity: 8.67\n",
      "Average loss at step 6200: 1.934210 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.20\n",
      "Validation set perplexity: 8.76\n",
      "Average loss at step 6300: 1.939020 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.30\n",
      "Validation set perplexity: 8.71\n",
      "Average loss at step 6400: 1.923438 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.05\n",
      "Validation set perplexity: 8.62\n",
      "Average loss at step 6500: 1.953911 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.14\n",
      "Validation set perplexity: 8.93\n",
      "Average loss at step 6600: 1.982936 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.78\n",
      "Validation set perplexity: 8.70\n",
      "Average loss at step 6700: 1.971051 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.49\n",
      "Validation set perplexity: 8.80\n",
      "Average loss at step 6800: 1.993616 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.46\n",
      "Validation set perplexity: 8.67\n",
      "Average loss at step 6900: 1.965390 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.63\n",
      "Validation set perplexity: 8.92\n",
      "Average loss at step 7000: 1.963907 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.77\n",
      "================================================================================\n",
      "swism groma worreny noot wents sartli mineoatsyny etoves lows usocws one two zerl\n",
      "pt cancutord giballersent niwnpsyumy typisedtring vilse que amessian stora aso is\n",
      "hjprecoez yohwenly one nine six with one six nine dyne h the lone ausydevity pica\n",
      "tihhepinaligove las thisea irnort inclervassicel the beused in dugag th raraoalar\n",
      "uture parreman a ciessions goencaricie whible arwour hamut and ble naimpactonk of\n",
      "================================================================================\n",
      "Validation set perplexity: 8.85\n",
      "Average loss at step 7100: 1.958827 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.72\n",
      "Validation set perplexity: 8.61\n",
      "Average loss at step 7200: 1.953741 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.61\n",
      "Validation set perplexity: 8.72\n",
      "Average loss at step 7300: 1.965292 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.06\n",
      "Validation set perplexity: 8.68\n",
      "Average loss at step 7400: 1.977342 learning rate: 1.000000\n",
      "Minibatch perplexity: 8.05\n",
      "Validation set perplexity: 8.38\n",
      "Average loss at step 7500: 1.990674 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.94\n",
      "Validation set perplexity: 8.40\n",
      "Average loss at step 7600: 1.955874 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.78\n",
      "Validation set perplexity: 8.65\n",
      "Average loss at step 7700: 1.959699 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.76\n",
      "Validation set perplexity: 8.67\n",
      "Average loss at step 7800: 1.981612 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.48\n",
      "Validation set perplexity: 8.32\n",
      "Average loss at step 7900: 1.984777 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.12\n",
      "Validation set perplexity: 8.46\n",
      "Average loss at step 8000: 2.003664 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.84\n",
      "================================================================================\n",
      "jztosa fou jo mace oftitalistror patect intunt whos rigrommussistever two nine me\n",
      "ph coloned  rome bpoch ulter grouralsed stnoy uly the mination aintrica one nine \n",
      "pnnation dimener the gralsailunciates p and to pulcust giate suted rughdkrtionwin\n",
      "spraces contion and beuw were gissicallifount all ez evotheurs al fule and ed the\n",
      "tdlamench famed he actorferonly promres and werre it fhgy but one nine one with o\n",
      "================================================================================\n",
      "Validation set perplexity: 8.69\n",
      "Average loss at step 8100: 1.976359 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.21\n",
      "Validation set perplexity: 8.33\n",
      "Average loss at step 8200: 1.958423 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.92\n",
      "Validation set perplexity: 8.54\n",
      "Average loss at step 8300: 1.968532 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.34\n",
      "Validation set perplexity: 8.09\n",
      "Average loss at step 8400: 1.978549 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.59\n",
      "Validation set perplexity: 8.44\n",
      "Average loss at step 8500: 1.984648 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.86\n",
      "Validation set perplexity: 8.51\n",
      "Average loss at step 8600: 1.968063 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.97\n",
      "Validation set perplexity: 8.57\n",
      "Average loss at step 8700: 1.962095 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.15\n",
      "Validation set perplexity: 8.64\n",
      "Average loss at step 8800: 1.945859 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.07\n",
      "Validation set perplexity: 8.53\n",
      "Average loss at step 8900: 1.957023 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.07\n",
      "Validation set perplexity: 8.54\n",
      "Average loss at step 9000: 1.948351 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.40\n",
      "================================================================================\n",
      "amne was annok he doecalterate makens lact firiebn mepelliaa oter wholles where n\n",
      "qvste and apsed and the erver brasests obath as enteves germ tated the saainroday\n",
      "uee cernote wor the gausloptive words luced gotetitally of aat flowfl sie as the \n",
      "bgas he a alsely corran on ent fitem efention incle lolls or cause beiu cooter al\n",
      "xbunshed apons emper of the beut cal shomn irioned of the by to gub ds awarity ni\n",
      "================================================================================\n",
      "Validation set perplexity: 8.36\n",
      "Average loss at step 9100: 1.958555 learning rate: 1.000000\n",
      "Minibatch perplexity: 8.00\n",
      "Validation set perplexity: 8.38\n",
      "Average loss at step 9200: 1.989222 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.03\n",
      "Validation set perplexity: 8.66\n",
      "Average loss at step 9300: 1.991627 learning rate: 1.000000\n",
      "Minibatch perplexity: 8.29\n",
      "Validation set perplexity: 8.57\n",
      "Average loss at step 9400: 1.983995 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.62\n",
      "Validation set perplexity: 8.41\n",
      "Average loss at step 9500: 1.986915 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.75\n",
      "Validation set perplexity: 8.47\n",
      "Average loss at step 9600: 1.975509 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.00\n",
      "Validation set perplexity: 8.48\n",
      "Average loss at step 9700: 1.988574 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.57\n",
      "Validation set perplexity: 8.61\n",
      "Average loss at step 9800: 1.989865 learning rate: 1.000000\n",
      "Minibatch perplexity: 8.24\n",
      "Validation set perplexity: 8.39\n",
      "Average loss at step 9900: 1.973568 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.07\n",
      "Validation set perplexity: 8.62\n",
      "Average loss at step 10000: 1.988290 learning rate: 0.100000\n",
      "Minibatch perplexity: 7.21\n",
      "================================================================================\n",
      "cdidnuth one nine rish ver ampan a he tult octoerbar com ded ara and of caired ma\n",
      "vv cliss the mees an coinece   d javated by the plirdry that be the coctable tlat\n",
      "nyd and whobire brimeljypas segive a one confitteslsocallys ass fase traint epitt\n",
      "fqiron a stand in with selates erd cond mrre med ther gion aspi orip uction than \n",
      "mft it asrro ba thael thearlayexeques to ovhisetteen balrlonage thac likorn rount\n",
      "================================================================================\n",
      "Validation set perplexity: 8.69\n",
      "Average loss at step 10100: 1.963723 learning rate: 0.100000\n",
      "Minibatch perplexity: 7.13\n",
      "Validation set perplexity: 8.63\n",
      "Average loss at step 10200: 1.954187 learning rate: 0.100000\n",
      "Minibatch perplexity: 6.06\n",
      "Validation set perplexity: 8.46\n",
      "Average loss at step 10300: 1.963751 learning rate: 0.100000\n",
      "Minibatch perplexity: 7.29\n",
      "Validation set perplexity: 8.59\n",
      "Average loss at step 10400: 1.960292 learning rate: 0.100000\n",
      "Minibatch perplexity: 6.52\n",
      "Validation set perplexity: 8.69\n",
      "Average loss at step 10500: 1.991535 learning rate: 0.100000\n",
      "Minibatch perplexity: 6.50\n",
      "Validation set perplexity: 8.45\n",
      "Average loss at step 10600: 1.978492 learning rate: 0.100000\n",
      "Minibatch perplexity: 7.48\n",
      "Validation set perplexity: 8.45\n",
      "Average loss at step 10700: 1.986652 learning rate: 0.100000\n",
      "Minibatch perplexity: 6.93\n",
      "Validation set perplexity: 8.50\n",
      "Average loss at step 10800: 1.987075 learning rate: 0.100000\n",
      "Minibatch perplexity: 7.17\n",
      "Validation set perplexity: 8.40\n",
      "Average loss at step 10900: 1.986672 learning rate: 0.100000\n",
      "Minibatch perplexity: 6.68\n",
      "Validation set perplexity: 8.43\n",
      "Average loss at step 11000: 1.979414 learning rate: 0.100000\n",
      "Minibatch perplexity: 7.15\n",
      "================================================================================\n",
      "fqm line nine three of imockrhenes the by condeprosman confliliet or sion ewesion\n",
      "ures they the sday two satote of the oli condobever les inbution syleents johowve\n",
      "jld gfsotions oritattecnlime eciand thein form that aunney shnal one two exeyin c\n",
      "vxmo vicd that jria seven king d noted one and is peagenor cart and lam ain of th\n",
      "wms thido of actional chund af braodud sting inses a fromandie alnate aboe the th\n",
      "================================================================================\n",
      "Validation set perplexity: 8.46\n",
      "Average loss at step 11100: 1.967816 learning rate: 0.100000\n",
      "Minibatch perplexity: 6.99\n",
      "Validation set perplexity: 8.51\n",
      "Average loss at step 11200: 1.949858 learning rate: 0.100000\n",
      "Minibatch perplexity: 7.02\n",
      "Validation set perplexity: 8.54\n",
      "Average loss at step 11300: 1.962712 learning rate: 0.100000\n",
      "Minibatch perplexity: 7.45\n",
      "Validation set perplexity: 8.50\n",
      "Average loss at step 11400: 1.942984 learning rate: 0.100000\n",
      "Minibatch perplexity: 6.60\n",
      "Validation set perplexity: 8.36\n",
      "Average loss at step 11500: 1.988169 learning rate: 0.100000\n",
      "Minibatch perplexity: 7.44\n",
      "Validation set perplexity: 8.52\n",
      "Average loss at step 11600: 1.985163 learning rate: 0.100000\n",
      "Minibatch perplexity: 6.42\n",
      "Validation set perplexity: 8.36\n",
      "Average loss at step 11700: 1.980171 learning rate: 0.100000\n",
      "Minibatch perplexity: 7.87\n",
      "Validation set perplexity: 8.33\n",
      "Average loss at step 11800: 1.980754 learning rate: 0.100000\n",
      "Minibatch perplexity: 7.70\n",
      "Validation set perplexity: 8.68\n",
      "Average loss at step 11900: 1.979335 learning rate: 0.100000\n",
      "Minibatch perplexity: 6.66\n",
      "Validation set perplexity: 8.39\n",
      "Average loss at step 12000: 1.970823 learning rate: 0.100000\n",
      "Minibatch perplexity: 6.51\n",
      "================================================================================\n",
      "halled dissovander of tharo doels hypucs lusher the flos a nefleyoyjki somer trat\n",
      "mf s pone the ernaid the the romuct nhcmgolialles havs buation whogy mind d eight\n",
      "ce and ithinese s arhpbnenton bunat zero two me nere one nine sen st wilwer islal\n",
      "gxneral thes a s even he roosfhssiss ond the meotary gare thseo an farlhad one ei\n",
      "qntoran moket welgench hall reingiion but whact prop attlaemit eme mayandine befh\n",
      "================================================================================\n",
      "Validation set perplexity: 8.48\n",
      "Average loss at step 12100: 1.958986 learning rate: 0.100000\n",
      "Minibatch perplexity: 8.27\n",
      "Validation set perplexity: 8.54\n",
      "Average loss at step 12200: 1.968943 learning rate: 0.100000\n",
      "Minibatch perplexity: 7.43\n",
      "Validation set perplexity: 8.43\n",
      "Average loss at step 12300: 1.970803 learning rate: 0.100000\n",
      "Minibatch perplexity: 6.98\n",
      "Validation set perplexity: 8.64\n",
      "Average loss at step 12400: 1.977375 learning rate: 0.100000\n",
      "Minibatch perplexity: 7.12\n",
      "Validation set perplexity: 8.68\n",
      "Average loss at step 12500: 1.973476 learning rate: 0.100000\n",
      "Minibatch perplexity: 6.69\n",
      "Validation set perplexity: 8.56\n",
      "Average loss at step 12600: 1.957847 learning rate: 0.100000\n",
      "Minibatch perplexity: 6.93\n",
      "Validation set perplexity: 8.62\n",
      "Average loss at step 12700: 1.956063 learning rate: 0.100000\n",
      "Minibatch perplexity: 6.79\n",
      "Validation set perplexity: 8.44\n",
      "Average loss at step 12800: 1.977918 learning rate: 0.100000\n",
      "Minibatch perplexity: 7.14\n",
      "Validation set perplexity: 8.47\n",
      "Average loss at step 12900: 1.963052 learning rate: 0.100000\n",
      "Minibatch perplexity: 6.16\n",
      "Validation set perplexity: 8.80\n",
      "Average loss at step 13000: 1.957582 learning rate: 0.100000\n",
      "Minibatch perplexity: 7.38\n",
      "================================================================================\n",
      "zm amplukilds eight fwas coui it dom with irerge rerlflyuo yal cibaks tenaagal bi\n",
      "ks that unstudy is to two by the thes codrdent irothe fadnasi one eelanved ropul \n",
      " pine drace in thod howle with of whinzigh ea reme one in undian one nimen s wa f\n",
      "qweolop uder of the motcasint   vestder will a tecte tbagend the stensicor in eyi\n",
      "qjucer deferoly lex cflant curla ctors entet segoscialan the one for envoe rescon\n",
      "================================================================================\n",
      "Validation set perplexity: 8.46\n",
      "Average loss at step 13100: 1.958418 learning rate: 0.100000\n",
      "Minibatch perplexity: 6.64\n",
      "Validation set perplexity: 8.44\n",
      "Average loss at step 13200: 1.955997 learning rate: 0.100000\n",
      "Minibatch perplexity: 7.46\n",
      "Validation set perplexity: 8.58\n",
      "Average loss at step 13300: 1.967854 learning rate: 0.100000\n",
      "Minibatch perplexity: 8.34\n",
      "Validation set perplexity: 8.65\n",
      "Average loss at step 13400: 1.963077 learning rate: 0.100000\n",
      "Minibatch perplexity: 6.81\n",
      "Validation set perplexity: 8.34\n",
      "Average loss at step 13500: 1.936476 learning rate: 0.100000\n",
      "Minibatch perplexity: 7.77\n",
      "Validation set perplexity: 8.67\n",
      "Average loss at step 13600: 1.988337 learning rate: 0.100000\n",
      "Minibatch perplexity: 6.73\n",
      "Validation set perplexity: 8.58\n",
      "Average loss at step 13700: 1.964716 learning rate: 0.100000\n",
      "Minibatch perplexity: 7.16\n",
      "Validation set perplexity: 8.65\n",
      "Average loss at step 13800: 1.975943 learning rate: 0.100000\n",
      "Minibatch perplexity: 6.91\n",
      "Validation set perplexity: 8.70\n",
      "Average loss at step 13900: 1.960464 learning rate: 0.100000\n",
      "Minibatch perplexity: 6.30\n",
      "Validation set perplexity: 8.52\n",
      "Average loss at step 14000: 1.979255 learning rate: 0.100000\n",
      "Minibatch perplexity: 8.18\n",
      "================================================================================\n",
      "jjity deast busemfulcrat town the gricaratholier condoncy crh in then inver remen\n",
      "sam raney unitic contrand hokey of porting in wwylinfink be ciecth relisit ust ru\n",
      "gcropesid screies houth its one one three eight two one asuled dafside geiyrochan\n",
      "oimurs quhkan in guance thit praveire in nupbutienda this fungs for kigs stented \n",
      "tions offic has be a ong sefterned isnvenet regist biknaolic to aftenter quest a \n",
      "================================================================================\n",
      "Validation set perplexity: 8.54\n",
      "Average loss at step 14100: 1.977074 learning rate: 0.100000\n",
      "Minibatch perplexity: 7.95\n",
      "Validation set perplexity: 8.65\n",
      "Average loss at step 14200: 1.960583 learning rate: 0.100000\n",
      "Minibatch perplexity: 6.74\n",
      "Validation set perplexity: 8.37\n",
      "Average loss at step 14300: 1.938979 learning rate: 0.100000\n",
      "Minibatch perplexity: 7.79\n",
      "Validation set perplexity: 8.60\n",
      "Average loss at step 14400: 1.943873 learning rate: 0.100000\n",
      "Minibatch perplexity: 7.05\n",
      "Validation set perplexity: 8.62\n",
      "Average loss at step 14500: 1.987275 learning rate: 0.100000\n",
      "Minibatch perplexity: 6.72\n",
      "Validation set perplexity: 8.75\n",
      "Average loss at step 14600: 1.959595 learning rate: 0.100000\n",
      "Minibatch perplexity: 7.27\n",
      "Validation set perplexity: 8.58\n",
      "Average loss at step 14700: 1.974307 learning rate: 0.100000\n",
      "Minibatch perplexity: 7.04\n",
      "Validation set perplexity: 8.43\n",
      "Average loss at step 14800: 1.978679 learning rate: 0.100000\n",
      "Minibatch perplexity: 6.45\n",
      "Validation set perplexity: 8.26\n",
      "Average loss at step 14900: 1.986614 learning rate: 0.100000\n",
      "Minibatch perplexity: 6.91\n",
      "Validation set perplexity: 8.28\n",
      "Average loss at step 15000: 1.939988 learning rate: 0.010000\n",
      "Minibatch perplexity: 6.47\n",
      "================================================================================\n",
      "ks snortandive in hastorsy it in for in me brureus trusedinge ressyno zero herde \n",
      "xjabled wimer they atton noom te dout falling hoose or rempartionivot wted act no\n",
      "mals comesnyre of the requatone expalms as the sessing sevuide mits ditprum side \n",
      "zkdtucys uce beerm itts one cats of the vardieptes in phanah eight ed of theact b\n",
      "worthbt tiled waloa bran ofter coursotp of pet the was eumek croinitides to ninga\n",
      "================================================================================\n",
      "Validation set perplexity: 8.38\n"
     ]
    }
   ],
   "source": [
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 2)\n",
    "\n",
    "num_steps = 15001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[2:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = []\n",
    "          feed.append(sample(random_distribution()))\n",
    "          feed.append(sample(random_distribution()))\n",
    "          sentence = characters(feed[0])[0] + characters(feed[1])[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input[0]: feed[0],\n",
    "                                                sample_input[1]:feed[1]})\n",
    "            feed = [feed[1], sample(prediction)]\n",
    "            sentence += characters(feed[1])[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input[0]: b[0],\n",
    "                                                sample_input[1]:b[1]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[2])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
